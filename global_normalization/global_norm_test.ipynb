{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                        roc_curve, auc, roc_auc_score, log_loss)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from essentials import complete_preprocessing_pipeline\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from essentials import normalization\n",
    "from feature_sets_center_less import GenerateFeatures\n",
    "from two_class_ncv_selectk import run_modified_nested_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/df_dict_imu.pkl', 'rb') as f:\n",
    "    imu_dict = pickle.load(f)\n",
    "with open('../data/df_dict_urineestimate_method1.pkl', 'rb') as f:\n",
    "    urine_estimates_dict = pickle.load(f)\n",
    "with open('../data/df_minze_dict.pkl', 'rb') as f:\n",
    "    ground_truth_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imu_dict['subj_9_void4']\n",
    "del imu_dict['subj_11_void2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = copy.deepcopy(imu_dict)\n",
    "labelled_imu_dict = complete_preprocessing_pipeline(data_dict, ground_truth_dict, \n",
    "                                target_fs=60,normalize_data=False, use_three_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30149800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add experiment_id to each dataframe and append to imu_dict\n",
    "imu_list = []\n",
    "for i, key in enumerate(labelled_imu_dict.keys()):\n",
    "    df  = labelled_imu_dict[key]\n",
    "    df['experiment_id'] = i + 1\n",
    "\n",
    "    imu_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8267a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes in imu_dict into a single dataframe\n",
    "main_df = pd.concat(imu_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window configurations (same as your pipeline)\n",
    "window_configs = [\n",
    "    (1, 0.0), (1, 0.5), (1, 0.8),\n",
    "    (2, 0.0), (2, 0.5), (2, 0.8),\n",
    "    (3, 0.0), (3, 0.5), (3, 0.8),\n",
    "    (4, 0.0), (4, 0.5), (4, 0.8),\n",
    "    (5, 0.0), (5, 0.5), (5, 0.8)\n",
    "]\n",
    "\n",
    "def dataframe_to_dict_by_experiment(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Convert DataFrame to dictionary with experiment_id as key\"\"\"\n",
    "    experiment_dict = {}\n",
    "    for exp_id in df['experiment_id'].unique():\n",
    "        exp_data = df[df['experiment_id'] == exp_id].copy()\n",
    "        exp_data = exp_data.drop(columns=['experiment_id'])\n",
    "        # IMPORTANT: Reset index to avoid KeyError in feature extraction\n",
    "        exp_data = exp_data.reset_index(drop=True)\n",
    "        experiment_dict[f'exp_{exp_id}'] = exp_data\n",
    "    return experiment_dict\n",
    "\n",
    "def extract_features_from_dict(data_dict: dict, window_size: float, overlap: float) -> pd.DataFrame:\n",
    "    \"\"\"Extract features from dictionary of DataFrames\"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for exp_key, df in data_dict.items():\n",
    "        actual_exp_id = int(exp_key.split('_')[1])\n",
    "        \n",
    "        analyzer = GenerateFeatures(fs=60, window_duration=window_size, overlap=overlap)\n",
    "        features, _ = analyzer.analyze_multi_axis_imu(df)\n",
    "        \n",
    "        table = analyzer.create_summary_table()\n",
    "        table['experiment_id'] = actual_exp_id\n",
    "        all_features.append(table)\n",
    "    \n",
    "    return pd.concat(all_features, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccdf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all window configurations\n",
    "all_global_norm_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310b7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outer_splits = 5\n",
    "n_inner_splits = 3\n",
    "\n",
    "outer_cv = StratifiedGroupKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedGroupKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop through each window configuration\n",
    "for window_size, overlap in window_configs:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING: {window_size}s window, {overlap} overlap\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    config_fold_results = []\n",
    "\n",
    "    # Split dictionary into training and testing sets based on void instances\n",
    "    for fold_id, (train_id, test_id) in enumerate(outer_cv.split(main_df, y=main_df['label'], groups=main_df['experiment_id'])):\n",
    "        print(f\"Fold {fold_id + 1}\")\n",
    "        data_train, data_test = main_df.iloc[train_id], main_df.iloc[test_id]\n",
    "        _, _ = main_df['label'].iloc[train_id], main_df['label'].iloc[test_id]\n",
    "        groups_train, groups_test = main_df['experiment_id'].iloc[train_id], main_df['experiment_id'].iloc[test_id]\n",
    "\n",
    "        \n",
    "        # Apply global normalization\n",
    "        data_train_norm, data_test_norm = normalization(data_train, data_test)\n",
    "\n",
    "        print(f\"✓ Applied global normalization\")\n",
    "        print(f\"Train experiments: {sorted(groups_train.unique())}\")\n",
    "        print(f\"Test experiments: {sorted(groups_test.unique())}\")\n",
    "    \n",
    "        # Convert normalized DataFrames to dictionaries for feature extraction\n",
    "        train_dict = dataframe_to_dict_by_experiment(data_train_norm)\n",
    "        test_dict = dataframe_to_dict_by_experiment(data_test_norm)\n",
    "        \n",
    "        # Extract features\n",
    "        print(f\"Extracting features...\")\n",
    "        train_features = extract_features_from_dict(train_dict, window_size, overlap)\n",
    "        test_features = extract_features_from_dict(test_dict, window_size, overlap)\n",
    "        \n",
    "        print(f\"Train features shape: {train_features.shape}\")\n",
    "        print(f\"Test features shape: {test_features.shape}\")\n",
    "        \n",
    "        # Store this fold's results\n",
    "        fold_result = {\n",
    "            'fold': fold_id + 1,\n",
    "            'window_size': window_size,\n",
    "            'overlap': overlap,\n",
    "            'train_features': train_features,\n",
    "            'test_features': test_features,\n",
    "            'original_train_groups': groups_train,\n",
    "            'original_test_groups': groups_test\n",
    "        }\n",
    "        config_fold_results.append(fold_result)\n",
    "        \n",
    "    # Store results for this configuration - for each window size and overlap, we get 5 fold results\n",
    "    overlap_str = 'no' if overlap == 0.0 else str(overlap)\n",
    "    config_key = f\"{window_size}s_{overlap_str}\"\n",
    "    all_global_norm_results[config_key] = config_fold_results\n",
    "    \n",
    "    print(f\"\\n✓ Completed {config_key}: {len(config_fold_results)} folds\")\n",
    "    \n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURE EXTRACTION COMPLETED FOR ALL CONFIGURATIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Show summary\n",
    "for config_key, fold_results in all_global_norm_results.items():\n",
    "    print(f\"\\n{config_key}:\")\n",
    "    for fold_result in fold_results:\n",
    "        fold_id = fold_result['fold']\n",
    "        train_shape = fold_result['train_features'].shape\n",
    "        test_shape = fold_result['test_features'].shape\n",
    "        print(f\"  Fold {fold_result['fold']}: Train{train_shape}, Test{test_shape}\")\n",
    "        \n",
    "        # Save train and test features to CSV for inspection\n",
    "        fold_result['train_features'].to_csv(\n",
    "            f'/home/edumaba/Public/MPhil_Thesis/Code/wear_uropatch/global_normalization/extracted_features/train_{config_key}_fold{fold_id}.csv', \n",
    "            index=False\n",
    "        )\n",
    "        fold_result['test_features'].to_csv(\n",
    "            f'/home/edumaba/Public/MPhil_Thesis/Code/wear_uropatch/global_normalization/extracted_features/test_{config_key}_fold{fold_id}.csv', \n",
    "            index=False\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56476200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the results for later use\n",
    "# with open('/home/edumaba/Public/MPhil_Thesis/Code/wear_uropatch/global_normalization/global_norm_extracted_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_global_norm_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8fcbd",
   "metadata": {},
   "source": [
    "# Nested CV with pre-split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modified optimizer\n",
    "\n",
    "# Run the modified nested CV on your extracted features\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STARTING MODIFIED NESTED CV WITH HYPERPARAMETER OPTIMIZATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Check what you have\n",
    "print(f\"Available configurations: {list(all_global_norm_results.keys())}\")\n",
    "print(f\"Total folds per configuration: {len(list(all_global_norm_results.values())[0])}\")\n",
    "\n",
    "# Run the evaluation\n",
    "detailed_results, summary_results, optimizer = run_modified_nested_cv(\n",
    "    all_global_norm_results,\n",
    "    positive_class=\"void\",\n",
    "    n_inner_folds=3,  # Same as your original\n",
    "    n_trials=50       # Same as your original\n",
    ")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results.to_csv(\n",
    "    '/home/edumaba/Public/MPhil_Thesis/Code/wear_uropatch/global_normalization/nested_cv_results/global_norm_detailed_results.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Save summary results\n",
    "summary_results.to_csv(\n",
    "    '/home/edumaba/Public/MPhil_Thesis/Code/wear_uropatch/global_normalization/nested_cv_results/global_norm_summary_results.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS SUMMARY - AVERAGES ACROSS 5 OUTER FOLDS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Verify we have 5 folds per configuration\n",
    "folds_per_config = detailed_results.groupby(['config', 'model'])['fold'].nunique()\n",
    "print(f\"Verification - Folds per config/model: {folds_per_config.iloc[0]} (should be 5)\")\n",
    "print(f\"Total evaluations: {len(detailed_results)} (should be {15 * 5 * 3} = 225)\")\n",
    "\n",
    "# Show best configurations with explicit fold averaging\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TOP 10 CONFIGURATIONS (Mean ± Std across 5 folds)\")\n",
    "print(f\"{'='*50}\")\n",
    "top_configs = summary_results.sort_values('f1_positive_mean', ascending=False).head(10)\n",
    "\n",
    "print(f\"{'Config':<12} {'Model':<4} {'F1(+)':<12} {'Accuracy':<12} {'AUC':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in top_configs.iterrows():\n",
    "    print(f\"{row['config']:<12} {row['model']:<4} \"\n",
    "          f\"{row['f1_positive_mean']:.3f}±{row['f1_positive_std']:.3f}  \"\n",
    "          f\"{row['accuracy_mean']:.3f}±{row['accuracy_std']:.3f}  \"\n",
    "          f\"{row['auc_mean']:.3f}±{row['auc_std']:.3f}\")\n",
    "\n",
    "# Show best model overall\n",
    "best_result = summary_results.loc[summary_results['f1_positive_mean'].idxmax()]\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BEST OVERALL RESULT (Mean ± Std across 5 folds)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Configuration: {best_result['config']}\")\n",
    "print(f\"Model: {best_result['model']}\")\n",
    "print(f\"F1 (positive): {best_result['f1_positive_mean']:.4f} ± {best_result['f1_positive_std']:.4f}\")\n",
    "print(f\"Accuracy: {best_result['accuracy_mean']:.4f} ± {best_result['accuracy_std']:.4f}\")\n",
    "print(f\"AUC: {best_result['auc_mean']:.4f} ± {best_result['auc_std']:.4f}\")\n",
    "print(f\"Precision (pos): {best_result['precision_positive_mean']:.4f} ± {best_result['precision_positive_std']:.4f}\")\n",
    "print(f\"Recall (pos): {best_result['recall_positive_mean']:.4f} ± {best_result['recall_positive_std']:.4f}\")\n",
    "\n",
    "# Model comparison (averaged across ALL configurations and folds)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"MODEL COMPARISON (Mean ± Std across all configs & folds)\")\n",
    "print(f\"{'='*50}\")\n",
    "model_stats = []\n",
    "for model in detailed_results['model'].unique():\n",
    "    model_data = detailed_results[detailed_results['model'] == model]\n",
    "    model_stats.append({\n",
    "        'Model': model,\n",
    "        'F1_Pos_Mean': model_data['f1_positive'].mean(),\n",
    "        'F1_Pos_Std': model_data['f1_positive'].std(),\n",
    "        'Accuracy_Mean': model_data['accuracy'].mean(),\n",
    "        'Accuracy_Std': model_data['accuracy'].std(),\n",
    "        'AUC_Mean': model_data['auc'].mean(),\n",
    "        'AUC_Std': model_data['auc'].std(),\n",
    "        'N_Evaluations': len(model_data)\n",
    "    })\n",
    "\n",
    "model_comparison_df = pd.DataFrame(model_stats)\n",
    "print(model_comparison_df.round(4))\n",
    "\n",
    "# Configuration comparison (averaged across models and folds)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TOP 5 WINDOW CONFIGURATIONS (Mean ± Std across models & folds)\")\n",
    "print(f\"{'='*50}\")\n",
    "config_stats = []\n",
    "for config in detailed_results['config'].unique():\n",
    "    config_data = detailed_results[detailed_results['config'] == config]\n",
    "    config_stats.append({\n",
    "        'Config': config,\n",
    "        'F1_Pos_Mean': config_data['f1_positive'].mean(),\n",
    "        'F1_Pos_Std': config_data['f1_positive'].std(),\n",
    "        'Accuracy_Mean': config_data['accuracy'].mean(),\n",
    "        'AUC_Mean': config_data['auc'].mean(),\n",
    "        'N_Evaluations': len(config_data)\n",
    "    })\n",
    "\n",
    "config_comparison_df = pd.DataFrame(config_stats).sort_values('F1_Pos_Mean', ascending=False)\n",
    "print(config_comparison_df.head().round(4))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(f\"Detailed results saved to: global_normalization/nested_cv_results/global_norm_detailed_results.csv\")\n",
    "print(f\"Summary results saved to: global_normalization/nested_cv_results/global_norm_summary_results.csv\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abc914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
